name: Model Architecture Tests

on: [push, pull_request]

jobs:
  test:
    name: Check Model Architecture Requirements
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Cache pip dependencies
      uses: actions/cache@v2
      with:
        path: |
          ~/.cache/pip
          data
        key: ${{ runner.os }}-pip-data-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-data-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install requests tqdm

    - name: Download MNIST Dataset
      run: |
        python -c "
        import os
        import requests
        import gzip
        from tqdm import tqdm
        
        def download_file(url, filename):
            response = requests.get(url, stream=True)
            total_size = int(response.headers.get('content-length', 0))
            block_size = 1024
            progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)
            
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            with open(filename, 'wb') as f:
                for data in response.iter_content(block_size):
                    progress_bar.update(len(data))
                    f.write(data)
            progress_bar.close()
        
        # Create data directory
        data_dir = '../data/MNIST/raw'
        os.makedirs(data_dir, exist_ok=True)
        
        # URLs for MNIST dataset
        base_url = 'https://ossci-datasets.s3.amazonaws.com/mnist'
        files = [
            'train-images-idx3-ubyte.gz',
            'train-labels-idx1-ubyte.gz',
            't10k-images-idx3-ubyte.gz',
            't10k-labels-idx1-ubyte.gz'
        ]
        
        # Download files
        for file in files:
            url = f'{base_url}/{file}'
            filename = os.path.join(data_dir, file)
            if not os.path.exists(filename):
                print(f'Downloading {file}...')
                download_file(url, filename)
                print(f'Successfully downloaded {file}')
        
        print('Dataset download completed!')
        "

    - name: Check Parameter Count
      run: |
        python -c "
        import torch
        from model import Net
        model = Net()
        total_params = sum(p.numel() for p in model.parameters())
        print(f'Total parameters: {total_params}')
        assert total_params < 20000, f'Model has {total_params} parameters, exceeding limit of 20,000'
        "

    - name: Check BatchNorm Usage
      run: |
        python -c "
        import torch
        from model import Net
        model = Net()
        has_bn = any(isinstance(m, torch.nn.BatchNorm2d) for m in model.modules())
        assert has_bn, 'Model does not use Batch Normalization'
        bn_count = sum(1 for m in model.modules() if isinstance(m, torch.nn.BatchNorm2d))
        print(f'Number of BatchNorm layers: {bn_count}')
        "

    - name: Check Dropout Usage
      run: |
        python -c "
        import torch
        from model import Net
        model = Net()
        has_dropout = any(isinstance(m, torch.nn.Dropout) for m in model.modules())
        assert has_dropout, 'Model does not use Dropout'
        dropout_count = sum(1 for m in model.modules() if isinstance(m, torch.nn.Dropout))
        print(f'Number of Dropout layers: {dropout_count}')
        "

    - name: Check GAP Usage
      run: |
        python -c "
        import torch
        from model import Net
        model = Net()
        has_gap = any(isinstance(m, torch.nn.AdaptiveAvgPool2d) for m in model.modules())
        assert has_gap, 'Model does not use Global Average Pooling'
        print('Global Average Pooling layer found')
        "

    - name: Run Training and Validation
      run: |
        python -c "
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torchvision import datasets, transforms
        from model import Net
        import os
        from tqdm import tqdm
        import sys
        
        def test(model, device, test_loader):
            model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    pred = output.argmax(dim=1, keepdim=True)
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    total += target.size(0)
            return 100. * correct / total
        
        def train_and_test():
            try:
                # Setup device
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                print(f'Using device: {device}')
                
                # Data loading with error handling
                transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize((0.1307,), (0.3081,))
                ])
                
                print('Loading MNIST dataset...')
                train_dataset = datasets.MNIST('../data', train=True, download=False, transform=transform)
                
                # Split training data
                train_size = 50000
                val_size = 10000
                train_dataset, val_dataset = torch.utils.data.random_split(
                    train_dataset, [train_size, val_size]
                )
                
                print('Creating data loaders...')
                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)
                val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1000)
                
                # Model setup
                print('Setting up model...')
                model = Net().to(device)
                optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=1)
                
                # Training
                best_acc = 0
                epochs_without_improvement = 0
                max_epochs = 20
                
                print('Starting training...')
                for epoch in range(1, max_epochs + 1):
                    model.train()
                    for batch_idx, (data, target) in enumerate(train_loader):
                        data, target = data.to(device), target.to(device)
                        optimizer.zero_grad()
                        output = model(data)
                        loss = nn.functional.nll_loss(output, target)
                        loss.backward()
                        optimizer.step()
                        
                        if batch_idx % 100 == 0:
                            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                                  f'({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}')
                    
                    # Validation
                    acc = test(model, device, val_loader)
                    print(f'Epoch {epoch}: Validation Accuracy: {acc:.2f}%')
                    
                    if acc > best_acc:
                        best_acc = acc
                        epochs_without_improvement = 0
                        torch.save(model.state_dict(), 'best_model.pt')
                    else:
                        epochs_without_improvement += 1
                    
                    scheduler.step(acc)
                    
                    if acc >= 99.4:
                        print(f'Reached target accuracy of 99.4% in {epoch} epochs')
                        assert epoch <= 20, f'Took {epoch} epochs, exceeding limit of 20'
                        return True
                    
                    if epochs_without_improvement >= 5:
                        print('Early stopping triggered')
                        break
                
                assert best_acc >= 99.4, f'Failed to achieve 99.4% accuracy. Best accuracy: {best_acc:.2f}%'
                return False
                
            except Exception as e:
                print(f'Error during training: {str(e)}')
                sys.exit(1)
        
        # Run training and testing
        print('Starting model training and validation...')
        success = train_and_test()
        if success:
            print('All accuracy and epoch requirements met!')
        else:
            sys.exit(1)
        "